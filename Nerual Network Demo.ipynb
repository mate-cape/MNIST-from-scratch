{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mnist_loader\n",
    "from utils import show, barplot, accuracy, misclassified, inference\n",
    "from matplotlib import pyplot as plt\n",
    "import IPython\n",
    "from IPython.display import Image\n",
    "from random import randint\n",
    "Image('img/00.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "x_train, t_train, x_test, t_test = mnist_loader.load()\n",
    "\n",
    "print('Training set size: [{},{}]'.format(*x_train.shape))\n",
    "print('Test set size: [{},{}]'.format(*x_test.shape))\n",
    "_ = plt.pie([x_train.shape[0], x_test.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = 7000\n",
    "show(x_train[sample, :])\n",
    "print('Digit: {}'.format(t_train[sample]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuron\n",
    "Image('img/01.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(activation):\n",
    "    return 1 / (1 + np.exp(-activation))\n",
    "\n",
    "sigmoid(-0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoiding running into overflow\n",
    "def sigmoid(a):\n",
    "    a[a>=0] = 1 / (1 + np.exp(-a[a>=0]))\n",
    "    a[a<0] = np.exp(a[a<0]) / (np.exp(a[a<0]) + 1)\n",
    "    return a\n",
    "sigmoid(np.float32([-800]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer\n",
    "Image('img/02.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network\n",
    "Image('img/03.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \n",
    "        self.Wih = self.initialize_weights([input_size, hidden_size])\n",
    "        self.Bih = np.zeros([1, hidden_size])\n",
    "        self.Who = self.initialize_weights([hidden_size, output_size])\n",
    "        self.Bho = np.zeros([1, output_size])\n",
    "        \n",
    "        num_param = self.Wih.size + self.Bih.size + self.Who.size + self.Bho.size\n",
    "        print('\\nNetwork initialized with {} parameters.'.format(num_param))\n",
    "        \n",
    "        self.hidden_state = None\n",
    "        self.input = None\n",
    "    \n",
    "    @staticmethod\n",
    "    def initialize_weights(shape):\n",
    "        # initialization with random uniform distribution between [-1, 1]\n",
    "        return np.random.random_sample(shape) * 2 - 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess(input):\n",
    "        # shifting pixels intensities [0, 255] to [-1, 1]\n",
    "        return input/127.5 - 1\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.preprocess(input)\n",
    "        \n",
    "        z = x.dot(self.Wih) + self.Bih  # hidden activation\n",
    "        h = sigmoid(z)                  # hidden state\n",
    "        l = h.dot(self.Who) + self.Bho  # output activations\n",
    "        y = softmax(l)                  # output\n",
    "\n",
    "\n",
    "        # store some values for backprop\n",
    "        self.hidden_state = h\n",
    "        self.input = x\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(activation):\n",
    "    exponentiated = np.exp(activation)\n",
    "    return exponentiated/np.sum(exponentiated)\n",
    "\n",
    "activation = np.array([[0.5, 0.1, 1, 0.2]])\n",
    "y = softmax(activation)\n",
    "barplot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfortunately we need to complicate it a bit: numerical stability issues and handling batch inputs\n",
    "def softmax(activation):\n",
    "    # subtracting max value for numerical stability (avoiding exponentiating large values)\n",
    "    max_value = np.max(activation, axis=1)\n",
    "    max_value = np.repeat(np.expand_dims(max_value, 1), activation.shape[1], axis=1)\n",
    "    offset_values = activation - max_value\n",
    "    exponentiated = np.exp(offset_values)\n",
    "    sum_exp = np.repeat(np.expand_dims(np.sum(exponentiated, axis=1), 1), activation.shape[1], axis=1)\n",
    "    return exponentiated/sum_exp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create our network\n",
    "net = Net(input_size=784, hidden_size=30, output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try running forward propagation on arbitrary input image\n",
    "random_input = randint(0, 60000)\n",
    "input = x_train[random_input, :]\n",
    "output = net.forward(input)\n",
    "\n",
    "# plot input image and output distribution\n",
    "target = t_train[random_input]\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "show(input, ax1)\n",
    "barplot(output, ax2)\n",
    "_ = ax1.set_title('Input: {}'.format(target))\n",
    "_ = ax2.set_title('Output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check how well we do on the test set:\n",
    "acc = accuracy(x_test, t_test, net)\n",
    "print('Accuracy: {:02}%'.format(acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning\n",
    "Image('img/04_grad_desc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(target, output):\n",
    "    tiny = 1e-40\n",
    "    log_outputs = np.log(output + tiny)\n",
    "    loss = -target.transpose().dot(log_outputs)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play a bit with Cross-Entropy error\n",
    "cross_entropy_error(target=np.array([1, 0, 0]), output=np.array([1, 0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are re-shaping the training set to mini-batches for Stochastic Gradient Descent\n",
    "\n",
    "batch_size = 64\n",
    "num_batches = x_train.shape[0] // batch_size\n",
    "batches = np.reshape(x_train[:num_batches * batch_size, :], [num_batches, batch_size, -1])\n",
    "\n",
    "# also, for the loss calculation we need to encode targets as one-hot vectors\n",
    "identity_matrix = np.eye(10)\n",
    "onehot_encoding = identity_matrix[t_train]\n",
    "target_batches = np.reshape(onehot_encoding[:num_batches * batch_size, :], [num_batches, batch_size, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's add backprop to our network\n",
    "class Net:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        # initialize parameters\n",
    "        self.Wih = np.random.random_sample([input_size, hidden_size]) * 2 - 1\n",
    "        self.Bih = np.zeros([1, hidden_size])\n",
    "        self.Who = np.random.random_sample([hidden_size, output_size]) * 2 - 1\n",
    "        self.Bho = np.zeros([1, output_size])\n",
    "        self.input = None\n",
    "        self.hidden_state = None\n",
    "        self.batch_size = None\n",
    "\n",
    "        self.d_Bho = None\n",
    "        self.d_Who = None\n",
    "        self.d_Bih = None\n",
    "        self.d_Wih = None\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.batch_size = input.shape[0]\n",
    "        self.input = input\n",
    "\n",
    "        hidden_activation = input.dot(self.Wih) + self.Bih\n",
    "        self.hidden_state = sigmoid(hidden_activation)\n",
    "        logits = self.hidden_state.dot(self.Who) + self.Bho\n",
    "        output = softmax(logits)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backprop(self, output, target):\n",
    "        \n",
    "        d_logits = target - output\n",
    "\n",
    "        self.d_Bho = np.ones(self.batch_size).dot(d_logits)\n",
    "        self.d_Who = self.hidden_state.transpose().dot(d_logits)\n",
    "\n",
    "        d_hidden_state = d_logits.dot(self.Who.transpose())\n",
    "\n",
    "        d_k = np.multiply(np.multiply(self.hidden_state, 1 - self.hidden_state), d_hidden_state)\n",
    "\n",
    "\n",
    "        self.d_Bih = np.ones(self.batch_size).dot(d_k)\n",
    "        self.d_Wih = self.input.transpose().dot(d_k)\n",
    "\n",
    "\n",
    "    def apply_gradients(self, learning_rate):\n",
    "\n",
    "        self.Wih += self.d_Wih * learning_rate\n",
    "        self.Bih += self.d_Bih * learning_rate\n",
    "        self.Who += self.d_Who * learning_rate\n",
    "        self.Bho += self.d_Bho * learning_rate\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(input_size=784, hidden_size=100, output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN!\n",
    "epochs = 10\n",
    "accuracies = []\n",
    "for j in range(epochs):\n",
    "    for i in range(num_batches):\n",
    "\n",
    "        input = batches[i] / 255\n",
    "        target = target_batches[i]\n",
    "\n",
    "        output = net.forward(input)\n",
    "        net.backprop(output, target)\n",
    "        net.apply_gradients(0.001)\n",
    "        \n",
    "\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            #print(mean_squared_error(target, output))\n",
    "            acc = accuracy(x_test, t_test, net) * 100\n",
    "            print('Epoch: {0} \\t accuracy: {1:3.2f}%'.format(j, acc))\n",
    "            accuracies.append(acc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try running forward propagation on arbitrary input image\n",
    "random_input = randint(0, 10000)\n",
    "input = x_test[random_input, :]\n",
    "output = net.forward(input)\n",
    "\n",
    "# plot input image and output distribution\n",
    "target = t_test[random_input]\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "show(input, ax1)\n",
    "barplot(output, ax2)\n",
    "ax1.set_title('Input: {}'.format(target))\n",
    "ax2.set_title('Output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed = misclassified(x_test, t_test, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = missed[100]\n",
    "input = x_test[sample, :]\n",
    "output = net.forward(input)\n",
    "\n",
    "# plot input image and output distribution\n",
    "target = t_test[sample]\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "show(input, ax1)\n",
    "barplot(output, ax2)\n",
    "ax1.set_title('Input: {}'.format(target))\n",
    "ax2.set_title('Output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
